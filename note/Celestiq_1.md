# 光线追踪
接下的部分就是构建一个能实时运行光追系统，预计会比较复杂，这一节来总览一下大概的技术路线

## 原理
我选择要实现的主要技术有两大块：**路径追踪和时域降噪**

首先基础肯定是一个没有各种优化的路径追踪系统，背景知识可参考：https://www.bilibili.com/video/BV1X7411F744 里的Ray-Tracing章节，最后的算法就是：

<img src="assets\C1_0.png" style="zoom:50%;" />

当然对于每个片段，可以选择采样N条光线，即执行N次这个shade()函数，N取得比较大的话，就是不考虑实时性能的光追算法了

想要做到实时性，实际上可以选择的优化方向有非常多，例如： SDF、Voxel Lighting、Radiosity、Irradiance Field、Radiance Cache、SSGI等等。作为一个初步的光追渲染器，提升最大，也是最直观的，当然是时域降噪（Temporal Filter）

即每个片段只执行1次shade()函数，然后做降噪，背景知识同样参考：https://www.bilibili.com/video/BV1YK4y1T7yY 里的Real-Time Ray-Tracing章节。这里的降噪实际上就是在时间上去积累上面的值N，一个场景完全不动时，就是在对一个片段多次采样，和上面的路径追踪一致，场景动起来时，用运动向量（motion vector）去尽量保持同一个片段的积累，保持不住的就会出各种问题，这个在闫老师的视频里看得也比较清楚了

## 获得硬件支持
只看上面的原理部分，在cpu端实现一个软光追其实非常简单，从前面的基础出发，要做的额外操作几乎只有光线求交，不考虑性能的话，cpu端我们可以非常自由的访问所有场景数据，用BVH优化一下求交过程，获取对应网格的材质信息，就搞定了

但如何在GPU端实现就比较复杂了。你会发现，只是额外的一个光线求交操作，就完全将光追管线和前面我们构建的管线（后文称为传统管线）完全分开。在上一章中构建的所有东西，都是基于传统的渲染管线的，OpenGL也是为此设计的，核心在于，使用OpenGL，只能在着色器中获取当前渲染的三角形的信息，以及屏幕空间信息，而光线求交这一步，必须有全部的场景信息才可以

和其他平台相比（Vulkan, DirectX），OpenGL的问题不仅是 API 功能缺失，而是设计原则的根本区别，基于 rasterization（栅格化）思想的系统，渲染流程是固定的：顶点 → 图元组装 → 光栅化 → 片元着色 → 输出。所有着色器都围绕“当前处理的几何体/像素”构建，永远访问不到“所有三角形”

严格来说，我们能靠 SSBO/Texture 模拟内存堆，来间接传入所有的场景信息，并且手动管理数据结构，然后使用GLSL来模拟一切，包括：构建 BVH；解析传递的场景数据；用栈模拟光线递归；手动材质采样逻辑等等，考虑到GLSL羸弱的语言特性（没有类和任何动态特性）和比较高的调试难度，开发会很艰难。当然，使用这样的方法构建的光追系统还是可以比cpu快上一些，几年前RT硬件加速还没完全普及的时候还有挺多人这样做的，但考虑到时代的发展，2025年光追单元都可以跑AI了，实在是没必要再复刻一遍

所以后面的开发基本上算是会重新开始，转向Vulkan平台。但前面的部分我认为还是有一些意义的，OpenGL是一个好的学习工具，从传统管线开始学习图形学还是绕不开的，而且就算是光追，传统管线依旧是有意义的，特别是当我们想实现屏幕空间效果时，也就是延迟渲染和G-buffer的思想，依旧是有用的


